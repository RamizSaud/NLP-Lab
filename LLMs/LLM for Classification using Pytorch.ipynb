{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1297ed5-fd63-4a4c-87c5-f278cfb83936",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e1614ea-ccec-4da2-b92b-4c854117bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012542fc-6ecd-4a59-b8ff-cd0e1eb92a95",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0f19e9-14ec-4075-a09e-4a5dfdec8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text and return tensors\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db86f7a-fc0e-450a-a0f9-5b21b80c32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"twitter_validation.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1464ffa1-4938-4992-81b5-391ae082afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([0,1], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5adc89d5-35a1-4699-b7c8-de64703321a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cfc8754-ae00-4b60-82dc-c92669e3f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c228f85-bf1f-4802-9d66-85ffdfd154ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels                                               text\n",
       "0    Irrelevant  I mentioned on Facebook that I was struggling ...\n",
       "1       Neutral  BBC News - Amazon boss Jeff Bezos rejects clai...\n",
       "2      Negative  @Microsoft Why do I pay for WORD when it funct...\n",
       "3      Negative  CSGO matchmaking is so full of closet hacking,...\n",
       "4       Neutral  Now the President is slapping Americans in the...\n",
       "..          ...                                                ...\n",
       "995  Irrelevant  ⭐️ Toronto is the arts and culture capital of ...\n",
       "996  Irrelevant  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...\n",
       "997    Positive  Today sucked so it’s time to drink wine n play...\n",
       "998    Positive  Bought a fraction of Microsoft today. Small wins.\n",
       "999     Neutral  Johnson & Johnson to stop selling talc baby po...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b3df197-958a-43f2-ab36-fe99c3bea663",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'Irrelevant': 3}\n",
    "df['labels'] = df['labels'].map(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73875cb3-5432-4ef7-b692-7c3308db32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf9a297-d1c3-4d1b-ba0d-d47e82ce0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2026d2c2-a067-4832-b9e6-9a860c8346ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(df_train['text'].tolist(), df_train['labels'].tolist(), tokenizer)\n",
    "val_dataset = TextDataset(df_val['text'].tolist(), df_val['labels'].tolist(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888988a1-fc55-4955-9514-51c740e1cba4",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a364561b-a8ff-417b-9282-142ea4fa29a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e49cec461f34c829cdc2b442d615ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Optionally, set up a learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc6a98-4262-4d7f-97eb-74ae7720b152",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "793b1fec-0d36-4189-b6c4-95cb782238d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training and validation functions\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5114ff9-64a8-49da-8dd9-f45fee17379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 57/57 [00:12<00:00,  4.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3273325091914128\n",
      "Validation Loss: 1.1927730526242937\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 57/57 [00:13<00:00,  4.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0794106115374649\n",
      "Validation Loss: 1.183968416282109\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 57/57 [00:13<00:00,  4.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7206382903090695\n",
      "Validation Loss: 1.2321644680840629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(3):  # number of epochs\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = eval_model(model, val_loader, device)\n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "    scheduler.step()  # Update the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9d9a4-e61b-4f6a-b67f-e8cc2e93b863",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12f08922-07a2-49d8-a373-5094f2c36074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is loaded and set to evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7251678f-c22c-47d4-99f4-a417686b61b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.57\n",
      "Precision: 0.5599780219780219\n",
      "Recall: 0.57\n",
      "F1 Score: 0.5555990004997501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110ab36-6ab5-4777-8ae9-6a73e5f08d46",
   "metadata": {},
   "source": [
    "# Predict Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fd7ec59-f208-4976-9ce3-44c47d1c5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Here is the text I want to classify\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Move the tensor to the same device as the model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82a36970-404b-49bb-97bb-7c88e6e6a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "predicted_class_idx = probabilities.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ace1ee0d-ff03-4eb6-8166-32bd70fcb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Positive', 'Negative', 'Neutral', 'Irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "302522cf-cd6d-4384-bd95-0aab12e85fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[predicted_class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffad1c7-cb7b-47af-991f-72d5789d6bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
