{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555bc720-6d6d-46fe-97a8-0178a4628e69",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0fc942-950d-440f-80cb-c4df2192a7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e163cd-6967-457f-aaa5-638ff7814919",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb131490-3266-45a3-9ddc-b87b61ef84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Chocolate Chip Cookies: Combine 2 cups of flour, 1 cup of sugar, 1 cup of brown sugar, 1 tsp of baking soda, and a pinch of salt. Cream together with 1 cup of softened butter, add 2 eggs and 2 tsp of vanilla extract. Fold in 2 cups of chocolate chips. Drop spoonfuls on a baking sheet and bake at 350°F for 10 minutes until golden brown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da8cfb9-77a6-4d0a-ac0e-5a9a8b161f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chocolate Chip Cookies: Combine 2 cups of flour, 1 cup of sugar, 1 cup of brown sugar, 1 tsp of baking soda, and a pinch of salt. Cream together with 1 cup of softened butter, add 2 eggs and 2 tsp of vanilla extract. Fold in 2 cups of chocolate chips. Drop spoonfuls on a baking sheet and bake at 350°F for 10 minutes until golden brown.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "499d8993-29c9-430e-b066-1d93de5bc768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chocolate Chip Cookies: Combine 2 cups of flour, 1 cup of sugar, 1 cup of brown sugar, 1 tsp of baking soda, and a pinch of salt.',\n",
       " 'Cream together with 1 cup of softened butter, add 2 eggs and 2 tsp of vanilla extract.',\n",
       " 'Fold in 2 cups of chocolate chips.',\n",
       " 'Drop spoonfuls on a baking sheet and bake at 350°F for 10 minutes until golden brown.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72964471-7ffb-4ed1-bb5e-248928ae0ac9",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0273e84-044c-47d2-8818-ab2b80f7dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sent_tokenize(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7620fc37-5d9f-4a21-b958-2c70043ed312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chocolate', 'Chip', 'Cookies', ':', 'Combine', '2', 'cups', 'of', 'flour', ',', '1', 'cup', 'of', 'sugar', ',', '1', 'cup', 'of', 'brown', 'sugar', ',', '1', 'tsp', 'of', 'baking', 'soda', ',', 'and', 'a', 'pinch', 'of', 'salt', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3e3b6-f180-4615-ba96-beebfc0f9a45",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9600b48-eaec-4a58-8aa8-c741f7891a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chocolate', 'NNP'), ('Chip', 'NNP'), ('Cookies', 'NNPS'), (':', ':'), ('Combine', 'VB'), ('2', 'CD'), ('cups', 'NNS'), ('of', 'IN'), ('flour', 'NN'), (',', ','), ('1', 'CD'), ('cup', 'NN'), ('of', 'IN'), ('sugar', 'NN'), (',', ','), ('1', 'CD'), ('cup', 'NN'), ('of', 'IN'), ('brown', 'JJ'), ('sugar', 'NN'), (',', ','), ('1', 'CD'), ('tsp', 'NN'), ('of', 'IN'), ('baking', 'VBG'), ('soda', 'NN'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('pinch', 'NN'), ('of', 'IN'), ('salt', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "print(pos_tag(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f0a7e-faae-4900-bb91-707cd824d760",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af7f9f20-62a7-4a9f-bf56-bf05038df52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'your', 'm', 'hers', 'hadn', 'himself', 'yourself', 'a', 'had', 'own', \"hadn't\", 'have', 'she', 'weren', \"should've\", 'are', 'with', 'me', 'about', \"you're\", 'there', 'so', \"it's\", 'who', \"weren't\", 'most', 'ourselves', 'didn', \"wasn't\", 'for', 'wouldn', 'between', 'of', 'has', 'where', 'he', 'when', 'to', 've', 'don', 'above', 'at', 'then', 'once', \"didn't\", \"isn't\", 'll', 'aren', 'herself', 'hasn', \"don't\", 'does', 'all', 'that', 'very', 'the', \"aren't\", 'out', 'being', 'or', 'by', 'wasn', 'into', 'through', \"shouldn't\", 'other', 'here', 'not', 'yours', 'having', 'isn', 'both', 'no', 'its', 'while', 'is', 'mustn', 'again', 'after', 'his', 'now', 'can', 'themselves', 'ain', 'few', 'do', 'if', 'doesn', 'haven', \"won't\", \"mustn't\", 'him', 'shouldn', 'those', \"needn't\", \"mightn't\", 'ma', 'against', 'same', 'whom', 'it', 's', 'further', \"shan't\", 'been', 'am', 'd', 'off', \"haven't\", 'theirs', \"couldn't\", 'from', 'nor', 'too', 'doing', 'just', 'their', 're', 'what', 'them', 'this', 'some', 'should', 'an', \"doesn't\", 'was', 'but', 'my', 'as', 'during', 'in', \"you'll\", 'needn', 'and', \"she's\", 'itself', 'why', 'which', 'they', 'down', 'mightn', 't', 'our', 'shan', \"you'd\", 'than', 'until', \"hasn't\", 'will', \"that'll\", 'up', 'below', 'any', 'o', 'such', 'did', 'be', 'ours', 'y', 'we', 'each', 'couldn', 'were', 'her', 'only', 'won', 'you', 'on', \"you've\", 'over', 'because', \"wouldn't\", 'i', 'how', 'under', 'more', 'before', 'myself', 'yourselves', 'these'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16126d1-4eaa-4269-83b8-583c88f46ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chocolate', 'Chip', 'Cookies', ':', 'Combine', '2', 'cups', 'flour', ',', '1', 'cup', 'sugar', ',', '1', 'cup', 'brown', 'sugar', ',', '1', 'tsp', 'baking', 'soda', ',', 'pinch', 'salt', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d0404-8674-425c-8bc5-743f12a0528b",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea0475e9-7abc-4947-91a3-0431c5d4e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['there', 'are', 'sever', 'type', 'of', 'stem', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentence = \"There are several types of stemming algorithms.\"\n",
    "words = word_tokenize(sentence)\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5eae7-972b-4fe2-92a9-4d8801bee0f4",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10c52b56-d1e6-4ea9-bee9-05cad746d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['The', 'leaf', 'on', 'the', 'tree', 'are', 'falling', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"The leaves on the tree are falling.\"\n",
    "words = word_tokenize(sentence)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97078a5f-5612-4945-ad19-c4cb47893ab9",
   "metadata": {},
   "source": [
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c6a183b-befb-4db1-8812-12fbb64fa041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f31983c4-055f-4652-9667-3fc72bb29ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 7), match='name'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"my name is Ramiz.\"\n",
    "re.search(r\"\\bname\\b\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a66b9007-759c-47c6-90b2-284ac724e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"name\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbb336-7b70-4b3c-a1c9-3a3e065260b8",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c74f7bca-6e64-40a8-9b28-19aa3736c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ab3bdae-c03c-4c77-869f-c711870079b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'apple': 2, 'mango': 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruits = [\"apple\",\"mango\",\"apple\"]\n",
    "Counter(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207384e1-a9e5-413a-b92b-824a7d8288ad",
   "metadata": {},
   "source": [
    "# Most Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b172bfe-46a3-4969-8984-327aac07b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we word tokenize and then we remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de873c6a-90dc-4a33-971c-4a9ee2d9533f",
   "metadata": {},
   "source": [
    "# File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8ce3a-5588-4143-b247-da58ce238e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.txt\", 'r') as file:\n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c46f7f-e59f-4631-8930-0aa02c90b7f4",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185adcd2-a2c9-4b10-b22a-4e83391fd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_word(word):\n",
    "    count = 0\n",
    "    for file in file_BOW:\n",
    "        if word in file_BOW[file]:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def word_count(word,file):\n",
    "    count = 0\n",
    "    for w in file_BOW[file]:\n",
    "        if word == w:\n",
    "            count+=1\n",
    "    return count\n",
    "np.log(5/doc_word(word))*word_count(word,file)/len(file_BOW[file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49418ab0-4593-4315-96f5-1119e7d003a7",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abc345dd-e081-4c09-b987-846c7e9c8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa399e30-9a58-4bd5-8658-bb208cef53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3269b-2415-4a95-8fb5-4be69f006dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = model['queen']-model['woman']+model['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd19c94-4f54-4be9-96c5-3e0c3e95edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.most_similar(positive=[word_vector],topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dfa17-fe0f-44ea-bff7-9b72be5afbab",
   "metadata": {},
   "source": [
    "# Visualize Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f46a4-59bf-4253-a453-54f2572c6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = ['king', 'queen', 'man', 'woman', 'paris', 'berlin', 'france', 'germany', 'apple', 'microsoft', 'google', 'facebook']\n",
    "\n",
    "word_vectors = np.array([model[word] for word in words])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c17af4-589a-4f8a-a709-f93ff7623534",
   "metadata": {},
   "source": [
    "# Classification using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca58b6-36b3-43c9-a3a3-fccd0c209175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def document_vector(doc):\n",
    "    doc = [word for word in word_tokenize(doc.lower()) if word in model.key_to_index]\n",
    "    return np.mean(model[doc], axis=0) if doc else np.zeros(300)\n",
    "\n",
    "X_w2v = np.array([document_vector(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ca6c7-b86f-4de8-b7df-232ba759570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w2v, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "model_w2v = LogisticRegression(max_iter=1000)\n",
    "model_w2v.fit(X_train, y_train)\n",
    "\n",
    "predictions_w2v = model_w2v.predict(X_test)\n",
    "accuracy_w2v = accuracy_score(y_test, predictions_w2v)\n",
    "print(f\"Accuracy with Word2Vec embeddings: {accuracy_w2v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
