{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3713de04-7379-42b9-a065-680f04ac4d0f",
   "metadata": {},
   "source": [
    "# LSTM for 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c527798-46bf-4d43-8d7c-a4bf0517729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 9ms/step - loss: 0.6918 - accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6866 - accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6828 - accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6769 - accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6691 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6622 - accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6544 - accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6479 - accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6372 - accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6225 - accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "[[0.5412762 ]\n",
      " [0.55116785]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example NLP dataset\n",
    "texts = [\n",
    "    \"This is a positive review\",\n",
    "    \"That was a great movie\",\n",
    "    \"I didn't like the ending\",\n",
    "    \"The acting was superb\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 1]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Padding sequences to ensure equal length input\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 16, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_texts = [\n",
    "    \"I loved the plot\",\n",
    "    \"The movie was terrible\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "predictions = model.predict(padded_test_sequences)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5985f-f08e-42f4-a8a2-f4012af51cd3",
   "metadata": {},
   "source": [
    "# LSTMs for more than 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0219f908-58b8-48ee-b357-33b15cd2c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 52ms/step - loss: 1.0994 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0924 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0843 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.0762 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0706 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0603 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0550 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0439 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0326 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0186 - accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "[2 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example NLP dataset with three classes\n",
    "texts = [\n",
    "    \"This is a positive review\",\n",
    "    \"That was a great movie\",\n",
    "    \"I didn't like the ending\",\n",
    "    \"The acting was superb\",\n",
    "    \"The plot was confusing\",\n",
    "    \"The movie was average\"\n",
    "]\n",
    "\n",
    "labels = [2, 2, 0, 2, 1, 1]  # 0 for negative, 1 for neutral, 2 for positive sentiment\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Padding sequences to ensure equal length input\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "num_classes = len(set(labels))\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# Define the LSTM model for multi-class classification\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 16, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_texts = [\n",
    "    \"I loved the plot\",\n",
    "    \"The movie was average\",\n",
    "    \"The acting was terrible\"\n",
    "]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "predictions = model.predict(padded_test_sequences)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5836f9-0b22-4bfc-8a2d-63964c43ba07",
   "metadata": {},
   "source": [
    "# LSTMs for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06255efd-743b-4013-9aa5-bcfedb074243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 60ms/step - loss: 2.0788 - accuracy: 0.1429\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.0745 - accuracy: 0.2381\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0704 - accuracy: 0.2381\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.0662 - accuracy: 0.2381\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0617 - accuracy: 0.2381\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.0568 - accuracy: 0.2381\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0527 - accuracy: 0.2381\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.0470 - accuracy: 0.2381\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.0411 - accuracy: 0.2857\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.0348 - accuracy: 0.2857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f540ece9910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example POS tagging dataset\n",
    "texts = [\n",
    "    \"The cat is sitting on the mat\",\n",
    "    \"A dog is barking loudly\",\n",
    "    \"The bird flew away quickly\"\n",
    "]\n",
    "\n",
    "tags = [\n",
    "    \"DT NN VBZ VBG IN DT NN\",\n",
    "    \"DT NN VBZ VBG RB\",\n",
    "    \"DT NN VBD RB RB\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "tag_index = tag_tokenizer.word_index\n",
    "\n",
    "# Convert texts and tags to sequences\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "tag_sequences = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max([len(seq) for seq in text_sequences])\n",
    "padded_text_sequences = pad_sequences(text_sequences, maxlen=max_length, padding='post')\n",
    "padded_tag_sequences = pad_sequences(tag_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert tags to one-hot encoded format\n",
    "num_tags = len(tag_index) + 1  # Add 1 for padding tag\n",
    "padded_tag_sequences = tf.keras.utils.to_categorical(padded_tag_sequences, num_tags)\n",
    "\n",
    "# Define the LSTM model for POS tagging\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 16, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.Dense(num_tags, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_text_sequences, padded_tag_sequences, epochs=10, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82efe0de-447d-4083-998f-f2b89eb50d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "['nn', 'nn', 'UNKNOWN_TAG', 'UNKNOWN_TAG', 'UNKNOWN_TAG', 'UNKNOWN_TAG', 'UNKNOWN_TAG']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_text = \"The cat is sleeping peacefully\"\n",
    "test_text_sequence = tokenizer.texts_to_sequences([test_text])\n",
    "padded_test_text_sequence = pad_sequences(test_text_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "predictions = model.predict(padded_test_text_sequence)\n",
    "predicted_tags_indices = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Convert predicted indices to tags\n",
    "predicted_tags = []\n",
    "for idx_seq in predicted_tags_indices[0]:\n",
    "    if idx_seq > 0 and idx_seq in tag_index.values():\n",
    "        predicted_tags.append(list(tag_index.keys())[list(tag_index.values()).index(idx_seq)])\n",
    "    else:\n",
    "        predicted_tags.append(\"UNKNOWN_TAG\")  # Placeholder for out-of-range indices\n",
    "\n",
    "print(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dc20360-fa0f-47ae-a673-a6eeb25c03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "# Example annotated text data (word sequences and corresponding labels)\n",
    "texts = [\n",
    "    [\"The\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat\"],\n",
    "    [\"A\", \"dog\", \"is\", \"barking\", \"loudly\"],\n",
    "    [\"The\", \"bird\", \"flew\", \"away\", \"quickly\"]\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    [\"DT\", \"NN\", \"VBZ\", \"VBG\", \"IN\", \"DT\", \"NN\"],\n",
    "    [\"DT\", \"NN\", \"VBZ\", \"VBG\", \"RB\"],\n",
    "    [\"DT\", \"NN\", \"VBD\", \"RB\", \"RB\"]\n",
    "]\n",
    "\n",
    "# Create word-to-index and label-to-index dictionaries\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(set([word for text in texts for word in text]))}\n",
    "label_to_index = {label: idx for idx, label in enumerate(set([label for labels_seq in labels for label in labels_seq]))}\n",
    "num_words = len(word_to_index) + 1  # Add 1 for padding token\n",
    "num_labels = len(label_to_index)\n",
    "\n",
    "# Convert texts and labels to sequences of indices\n",
    "text_sequences = [[word_to_index[word] for word in text] for text in texts]\n",
    "label_sequences = [[label_to_index[label] for label in labels_seq] for labels_seq in labels]\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max([len(seq) for seq in text_sequences])\n",
    "padded_text_sequences = pad_sequences(text_sequences, maxlen=max_length, padding='post')\n",
    "padded_label_sequences = pad_sequences(label_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define the LSTM + CRF model for sequence labeling\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=num_words, output_dim=64, input_length=max_length)(input_layer)\n",
    "lstm_layer = LSTM(units=64, return_sequences=True)(embedding_layer)\n",
    "output_layer = TimeDistributed(Dense(num_labels, activation=None))(lstm_layer)\n",
    "crf_layer = CRF(num_labels)(output_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=crf_layer)\n",
    "\n",
    "# Compile the model with CRF layer\n",
    "model.compile(optimizer='adam', loss=model.layers[-1].loss, metrics=[model.layers[-1].accuracy])\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_text_sequences, padded_label_sequences, epochs=10, batch_size=2)\n",
    "\n",
    "# Save the model for inference\n",
    "model.save(\"lstm_crf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1f028-82cf-4102-ac6f-325299c664ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
